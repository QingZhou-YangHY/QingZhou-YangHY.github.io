[{"content":"2025/12/23 孩子们别怕，你们的老大最近在准备考试 + 补基础（专注theroy）\n所有的前沿热点基本上都是第一时间能拿到信息，从GLA, MiniMax, GLM4.7（包括慢脚）等等这些一直紧跟前沿\n欢迎和期待研究员们能与我一块进行研究，拒绝短平快，长期目标是ICML, NeurIPS, ICLR, 故事会就算了哈!\nHow to contact with me? yhy3868850350@gmail.com\n","date":"2025-12-23T14:46:00+08:00","permalink":"http://localhost:1313/p/%E6%B2%89%E6%B7%80%E6%9C%9F/","title":"沉淀期"},{"content":"【生成式人工智慧與機器學習導論2025】第3講：解剖大型語言模型representation Token Embedding 是在Layer 1输入的那个， Contextualized Embedding(也就是我们说的representation表征) 指的是经过Layer 1输出的那个.\nRepresentation Engineering, Activation Engineering, Activating Steering\u0026hellip;\nLogit Lens 对每一层进行 Unembedding ,可以看每一层的思考所对应的文字，窥探语言模型的思考过程\nPatchscopes 把一个向量（一个token/字）替换成一句话\nLayer解读（使用Transformer架构的解读） Layer 中还有 Layer.\n首先过 Self-attention Layer (attention layer), 考虑上下文就是因为 attention layer, 输出经过几个 Feed Forward Layer\n典中典，要求手撕 Attention Layer ,\n自己跟自己也要进行dot product\ndot product 叫 Attention weight, 要所有的 weight 过 Softmax\nPositional Embedding Llama 用 Rope , 旋转位置编码, 为了把位置的咨询加入到计算中\nMulti-head Attention 每一个 head 的作用不一样 目前的语言模型大多数都是 Causal Attention, 这样计算方便(Autogressive)\n实做环节 1 model.num_parameters()会告诉我们 model 的参数量 深度学习模型的参数通常以多个矩阵 (Matrix) 和向量 (Vector) 的形式存储。向量、矩阵等统称为张量(Tensor)\nLlama3 有28层，Gemma4B 有44层\n1 model.state_dict() 可以实际把参数拿出来看看 ","date":"2025-11-19T10:00:00+08:00","permalink":"http://localhost:1313/p/%E8%A7%A3%E5%89%96%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/","title":"解剖大型语言模型"},{"content":"2025/10/23 实习第一天\n2025/10/27 周六凌晨开始跑的论文复现，当时为了图速度就没有顾虑太多。今天学姐问我一些实验过程中的指标变化等等来对比idea的实验是否正常工作，只能回去手动复制tmux里面的内容。\n每跑一次实验都要记得能留日志就留日志，能上wandb/Swanlab就上，跑完的实验数据等等都要保存，留着。后续可能会用到。\n千万不要一味地追求速度，图省事！\n与此同时解决了submodule问题，具体的流程大致为删掉子仓库中的.git，移除本地缓存（觉得他是子module），从.gitmodule中移除（当然我本地没有）。\n之前也解决了关于代码冲突的问题（需要手动进行修改），比如在git pull中是常见问题，大致进一步又了解了，准备后面有时间写一个关于这种多人协作常出现的问题，要注意的事项以及如何解决\n","date":"2025-10-23T10:00:00+08:00","image":"http://localhost:1313/p/sjtu-intern/SJTU_hu_545a5411535f420f.jpg","permalink":"http://localhost:1313/p/sjtu-intern/","title":"SJTU-Intern"},{"content":"基于veRL框架并行训练的深入探究 process-overview 加载模型\nveRL使用Ray框架进行模型的调度\ninit_workers 初始化策略模型、参考模型、价值模型、奖励模型。这里面有些模型不会用到。很多都不会使用价值模型，奖励模型也都是基于规则的 训练过程\nget_data_batch generate_sequences(normal-rl和agent-rl) normal-rl：单轮的交互（prompt\u0026ndash;\u0026gt;model\u0026ndash;\u0026gt;response），这个流程就结束了\nagent-rl：多轮的交互（prompt\u0026ndash;\u0026gt;model\u0026ndash;\u0026gt;response\u0026ndash;\u0026gt;env（tool call,code exec ,etc）\u0026ndash;\u0026gt;model\u0026ndash;\u0026gt;response\u0026ndash;\u0026gt;\u0026hellip;）\nreward（通过自定义的奖励函数控制）不用去修改veRL框架，通过配置文件进行修改\nlog_probs（计算策略模型和参考模型输出token的概率）\nvalues(目前主流的rl算法基本上都在舍弃价值模型)\nadv(各rl算法之间的差异主要体现在adv的计算方式，此处需要针对不同的算法进行修改) compute_loss（normal-rl和agent-rl） normal-rl：无需和环境交互，response中token全部由模型生成，均参与损失计算\nagent-rl：需要获取观测结果，观测结果部分不是模型生成，计算损失时需要对其进行mask\nupdate actor\nDeepSpeed DeepSpeed官方文档👈官方文档\nDeepSpeed配置JSON👈使用只需要JSON配置文件\n【利用多張GPU訓練大型語言模型】 - YouTube👈李宏毅老师YouTube视频讲解（约一个小时）\nThe Ultra-Scale Playbook:Training LLMs on GPU Clusters👈并行训练高质参考资料\nhugging face经常有高质量实验总结，可以多关注一下 Batch Size Related Parameters train_batch_size = train_micro_batch_size_per_gpu * gradient_accumulation_steps * number of GPUs\ntrain_batch_size: [integer] 代表着one step.Example:32\ntrain_micro_batch_size_per_gpu: [integer] 一次更新的batch_size，所以叫micro_batch_size.\ngradient_accumulation_steps: [integer] 积累几次\n一开始会有batch-size个prompt去做rollout，每个prompt rollout出n个response，之后每mini-batch-size个prompt及其rollout出来的response会去做一次梯度下降，batch-size / mini-batch-size次梯度下降之后一个step结束\nMonitoring Module 可以使用TensorBoard,andb,Comet等，因为个人使用所以只介绍swanlab(wandb的国内镜像)\n注册登录，设置 project_name 和 experiment_name 就可以在电脑上/手机上看了\n很好用的监控平台！\nswanlab官方文档👈官方文档\nThe Ultra-Scale Playbook:Training LLMs on GPU Clusters Finding the Best Training Configuration Step 1: Fitting a training step in memory 首先我们思考如何把一个完整的模型放在我们的GPUs集群里面，通常有一下两种办法\nGPU-rich case 🤑 - when you have plenty of GPUs available:\nFor models under 10B parameters, you can use a single parallelism technique, e.g. tensor parallelism or ZeRO-3/DP with full recompute across 8 GPUs. 后面模型太大了就不提了，可以自行去资料中翻看 GPU-poor case 😭 - when you might be low on GPU resources:\nYou can enable full activation recomputation to trade some compute for memory (and train a bit more slowly). You can increase gradient accumulation to process larger batches with limited memory. Now that we have a first model instance training, we need to make sure we have the right batch size.\nStep 2: Achieving the target global batch size To increase our current global batch size:\nWe can scale up data parallelism or gradient accumulation steps. For long sequences, we can leverage context parallelism. To decrease our current global batch size:\nWe can reduce data parallelism in favor of other parallelization strategies. For long sequences, we can reduce context parallelism. OK, now we have the model running in the general configuration we want in terms of model size and batch size - but are we training it the fastest way? The final step is to work on optimizing throughput.\nStep 3: Optimizing training throughput We want to make sure the training is running as fast as possible so all our precious GPUs are well utilized at all times. As long as memory and communication aren\u0026rsquo;t bottlenecks, we can try the following:\nScale up tensor parallelism (using the fast intra-node bandwidth) until we reach a degree close to the node size, so that we can reduce other forms of parallelism. Increase data parallelism with ZeRO-3 while keeping the target batch size. When data parallelism communication starts to become a bottleneck, transition to using pipeline parallelism. Try scaling up different parallelisms one by one. Experiment with micro-batch sizes (mbs) to aim for an optimal balance between max global batch size, model size, compute, and communication. Benchmarking thousands of configurations 有 Heatmap 可以看Best Configurations\nLessons learned on benchmarking What looks simple in theory often requires careful attention to many moving parts in practice.\n","date":"2025-10-16T00:00:00Z","permalink":"http://localhost:1313/p/%E5%9F%BA%E4%BA%8Everl%E6%A1%86%E6%9E%B6%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E7%9A%84%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6/","title":"基于veRL框架并行训练的深入探究"},{"content":"解决方案👈 方法1就完全work! 注意版本冲突问题，如果不确定可以多问问之前工作的环境或者clone几个备份！ ","date":"2025-10-09T10:00:00+08:00","permalink":"http://localhost:1313/p/flash-attention%E5%AE%89%E8%A3%85%E6%9C%80%E9%80%9F%E4%BC%A0%E8%AF%B4/","title":"Flash-Attention安装最速传说"},{"content":"问题详情 + 解决方案👈 ","date":"2025-10-09T10:00:00+08:00","image":"http://localhost:1313/p/ray%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%9B%A0%E8%BF%9B%E7%A8%8B%E8%80%97%E5%B0%BD%E5%AF%BC%E8%87%B4ssh%E6%96%AD%E5%BC%80%E8%BF%9E%E6%8E%A5/Ray_hu_8d061ec42dec3d1e.jpg","permalink":"http://localhost:1313/p/ray%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%9B%A0%E8%BF%9B%E7%A8%8B%E8%80%97%E5%B0%BD%E5%AF%BC%E8%87%B4ssh%E6%96%AD%E5%BC%80%E8%BF%9E%E6%8E%A5/","title":"Ray分布式训练因进程耗尽导致SSH断开连接"},{"content":"重新认真地读一遍STILL3(LLMs慢思维技术报告III )，补充知识点\nAn Empirical Study on Eliciting and Improving R1-like Reasoning Models 慢思考模型仓库\n缺乏可验证问题的领域:如DeepSeek-R1,基于规则和训练的奖励模型的联合使用\n随着训练的进行，可以观察到三个主要特征：增加训练奖励、增加响应长度和涌现推理模式。些因素是扩大强化学习训练成功的关键指标。\n在本报告中，首先深入研究了强化学习设置对训练效果的影响。接下来，通过强化学习训练直接激励基础模型发展复杂的推理能力，观察到模型逐渐花费更多的时间“思考”并表现出高级推理行为（例如，验证或反思）。最后，为了进一步增强微调模型的推理能力，探索了强化学习和工具增强作为提高模型推理性能的策略，在小型（1.5B）和中型LLM（32B）中都取得了显著的改进。\non-policy learning strategy被证明是关键因素！\nresponse length是RL训练成功的重要指标，这是结果，不是原因\n设计专门的奖励函数来鼓励模型产生更长的响应可能会导致奖励黑客攻击等问题，这不能从本质上增强模型的推理能力\n无论是短CoT还是长CoT，还是蒸馏过的模型，强化学习都可以提高能力\n通过fine-tuning，LRM可以获得操纵外部工具的能力，从而提高模型的性能。这种能力只需少量高质量的训练实例即可激活。\n复现点这里找资源\n实验设置 训练框架 OpenRLHF和veRL\n骨干模型 各种版本的QWEN2.5模型\nDEEPSEK-R1-DISTILL系列的1.5B和32B QWEN2.5\n在微调模型上进行实验，微调数据由自己合成。\n训练数据 多样性 AIME,MATH,NuminaMath,Open Reasoner Zero\n可验证性 删除了多选题，证明题，概念题，开放式问题和有多个子问题的问题\n再把答案不可能的数据删掉\n困难程度 基于模型的过滤，QWEN-7B-INSTRUCT正确率过高或者零通过率的题目删掉\n最后剩下90k个examples\nReward Design 设计并验证了一组不同的奖励，并分析了它们对模型性能的影响，包括输出奖励、格式奖励、长度奖励和动作奖励。\n输出奖励评估 最终答案是否与ground truth匹配。如果答案正确，我们将奖励设置为1，否则设置为0。\n如果模型未能将其最终答案放入\\boxed{}中，则奖励设置为0。\n使用格式奖励来指导基础模型正确构建其响应。\n探索了新的辅助奖励，包括鼓励更长反应的长度奖励和激励复杂推理行为的行动奖励。\nEvaluation Benchmarks 评估了各种数学推理任务的模型性能，包括MATH-OAI[13]、AIME、Omni-MATH[17]、LiveAOP[18]和HMMT。\niveAOP利用AoPS论坛的帖子创建了一个由3863个示例组成的抗污染评估集。\nCompute Environment 实验主要在DataCanvas的旗舰计算编排平台Alaya NeW AI操作系统上进行。\nRL Experiments on the Base Model 直接将RL应用于预训练的基础模型进行实验，而不需要任何中间的SFT阶段。这种方法旨在探索LLM是否可以通过纯粹的RL驱动的自我提升来自主发展推理能力\n考察了四个关键维度 1.训练超参数的影响 2.比较不同的基础模型并将其与具有短CoT推理能力的微调模型进行对标，来分析骨干模型的效果 3.快速设计对RL训练中基础模型推理能力的影响 4.代表性推理模式（如验证或反射）的出现 Exploring the Settings of RL Training 重点分析两个关键方面的影响：超参数和训练提示。\nInfluence of Training Hyper-parameters Train Batch Size(TBS) TBS=128 v.s.1024\n更大的TBS可以显著提高训练效率，使模型在早期训练阶段能够快速提高性能。此外，与较小批量相比，较大批量的训练表现出更大的稳定性，训练指标的波动显著减少\nLearning Strategy: On-policy vs. Off-policy on-policy 鼓励更多的探索；在训练过程中，模型自然快速地增加了响应长度，而更新较少的非策略学习在长度增长方面遇到了瓶颈\nRollout Parameters 主要研究两个rollout parameters（rollout times和rollout temperature）\n更大的推出数量和更高的温度通常表示更大程度的探索\nCoefficient of KL Penalty 动态KL退火具有很好的综合性\nEffect of Backbone Models 上述实验基于QWEN2.5-7B。此外，还对较小的QWEN2.5-1.5B模型、监督微调QWEN2.5-7B-INSTRUCT模型和数学专用QWEN2.5-math-7B模型进行了实验。\n在上述设置下，我们的实验表明，与QWEN2.5-1.5B相比，QWEN2.5-7B表现出更强的探索能力，并且在强化学习训练中遵循与QWEN2.5.7B-INSTRUCT类似的趋势。\nImpact of the Prompt 使用两种基本模型（即QWEN2.5-1.5B和QWEN2.5-7B）和两种类型的提示进行实验\n第一种是短提示，类似于DeepSeek-R1-Zero中使用的提示。此外，为了更好地引出基础模型的推理能力，我们设计了一个新的提示，其中包括关于推理过程的详细说明，称为长提示\n这个新提示保留了对特定推理格式的要求，同时添加了对推理过程的全面描述。这包括在推理过程中可以应用的策略（例如，分析问题、总结发现）以及在整个过程中使用的推荐表达和词汇（例如，“等待”、“替代”）\n在本实验中将learning rate、train batch size、rollout temperature和number of rollout times设置为1×10−6、128、1.0和8，并执行on-policy训练策略。我们将KL penalty和entropy loss的系数设置为0.0，有效地消除了模型上的约束。这使得可以更清楚地观察到各种提示导致的性能差异\n实验结果 对于QWEN2.5-1.5B，在短提示下训练的模型在测试集上的性能高于在长提示上训练的模型。这可能是因为1.5B大小的基本模型容量相对有限，难以遵循详细提示中的复杂说明。\n当在不同的提示下训练时，7B大小的模型在下游任务上显示出类似的性能。然而，在长提示上训练的模型会产生更短的响应，这表明它通过遵守提示中提供的指导方针来学习更有效的推理\n因此，我们得出结论，更详细的提示可以引导模型更有效地思考，提高推理效率。然而，它们不一定能提高下游任务的性能。\n","date":"2025-10-02T00:00:00Z","permalink":"http://localhost:1313/p/still3%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/","title":"STILL3论文精读"},{"content":"A Survey of Reinforcement Learning for Large Reasoning Models Background RL在推进LLM能力的前沿方面取得了显著成功，特别是在解决数学和编码等复杂逻辑任务方面。因此，RL已成为将LLM转化为LRM的基础方法。\n需要探索提高强化学习向人工超级智能（ASI）的可扩展性的策略。\n回顾一下大致流程吧！\n简单讲解一下RL应用到语言模型的时候，这些概念映射到了哪里 Prompt/Task（x）：对应于初始状态或环境上下文，从数据分布中提取，对应于数据集D。\nPolicy (πθ):表示语言模型，它根据提示生成一个长度为T的序列，表示为y=（y1，…，yT）。\nState (st):定义为提示以及到目前为止生成的令牌，即st=（x，a1:t−1）。\nAction (at):在步骤t从动作空间A中选择的单元。根据粒度，动作可以是整个序列y（序列级）、∈V处的令牌（令牌级）或 片段\nTransition Dynamics (P):在LLM的上下文中，状态转换通常是确定的，因为st+1=[st，at]，其中[·，·]表示字符串连接。当状态包含EOS令牌时，策略将转换为终端状态，这意味着轨迹结束。\nReward (R(x, y) or rt):基于动作粒度进行分配，例如，轨迹末端的序列级别R（x，y），每个令牌的令牌级别rt=R（x、a1:t），或每个分段的步长级别rk=R（x、y（1:k））。\nReturn (G):提示x的整个轨迹y的累积奖励（通常在有限时间内γ=1）。它通过序列级奖励简化为单个标量R（x，y），否则按每个令牌/步骤聚合奖励\nFrontier Models 按时间顺序排列在三个主要方向上：LRM、agentic LRMs和多模态LRM。\n一个大型推理模型，OpenAI的o1[2024]系列，建立了将训练时间RL和测试时间计算扩展到更强大的推理能力的有效性，在数学、编码和科学基准测试方面取得了领先成果。\nDeepSeek的旗舰模型R1[2025a]是第一个在基准测试中与o1性能相匹配的开源模型。它采用多阶段训练管道来确保全面的模型能力，并探索了没有监督微调的纯RL路线（即Zero RL）。\n其他专有模型发布紧随其后：Claude-3.7-Sonnet[2025a]以混合推理为特色，Gemini 2.0和2.5[2025]引入了更长的上下文长度，Seed Thinking 1.5[2025b]以跨领域的泛化为特色，o3[2025a]系列展示了越来越先进的推理能力。最近，OpenAI推出了他们的第一个开源推理模型gpt-oss-120b[2025a]，随后推出了GPT5[2025a]，这是他们迄今为止最强大的人工智能系统，可以在高效模型和更深入的推理模型gpt-5思维之间灵活切换。并行的开源努力继续扩大了格局。在Qwen家族中，QwQ-32B[2025g]与R1的表现相匹配，其次是Qwen3[2025a]系列，代表性型号Qwen3-235B进一步提高了基准分数。Skywork-OR1[2025d]模型套件基于R1蒸馏模型，并通过有效的数据混合和算法创新实现了可扩展的RL训练。Minimax-M1[2025a]是第一个有效地将混合注意力引入尺度RL的模型。其他作品包括Llama Nemotron Ultra[2025]，旨在平衡准确性和效率；Magistral 24B[2025]，通过RL从头开始训练，而不是从先前的模型中提炼；以及种子OSS[2025a]，强调长上下文推理能力。等等\u0026hellip;\n过去的一年发展迅速啊！\nFoundational Components Reward Design 在1.1中，我们对LRM RL中的奖励设计进行了全面的考察。从可验证的奖励开始，DeepSeek-R1的成功就是例证，它通过可验证的奖励机制证明了RL的可扩展性。\n在1.2中，我们考察生成性奖励，其中模型用于验证或直接生成奖励信号。\n然而，可验证和生成性奖励通常都表示为稀疏的数值反馈。一个重要的互补维度在于奖励信号的密度。\n1.3相应地考察了包含密集奖励的方法。另一个分类轴涉及奖励是根据外部真实情况计算的，还是由模型直接估计的。\n这一区别促使我们在1.4中讨论无监督奖励。\n在这四个类别的基础上，我们在1.5中转向奖励塑造，在那里我们分析了组合或转换不同奖励信号以促进学习的策略。\nVerifiable Rewards 基于规则的奖励通过利用准确性和格式检查，为RL提供可扩展和可靠的训练信号，特别是在数学和代码任务中。\nVerifier定律强调，具有清晰和自动验证的任务可以实现高效的RL优化，而主观任务仍然具有挑战性。\nRule-based Rewards Accuracy rewards:对于具有确定性结果的任务（例如数学），策略必须在规定的分隔符（通常为\\boxed{…}）内产生最终解决方案。然后，自动检查器将此输出与地面实况进行比较。对于编码任务，单元测试或编译器提供通过/失败信号\nFormat rewards:这些奖励施加了一个结构约束，要求模型将其私有思想链放置在和之间，并在单独的字段中输出最终答案（例如…）。这提高了大规模RL中的可靠解析和验证\nRule-based Verifier 基于规则的奖励通常来自基于规则的验证器。这些依赖于大量手动编写的等价规则来确定预测的答案是否与基本事实相匹配。目前，广泛使用的数学验证器主要基于Python库Math-Verify1和SymPy2构建。此外，一些作品，如DAPO[2025d]和DeepScaleR[2025c]，也提供了开源和成熟的验证器。最近，Huang等人[2025e]强调了与基于规则和基于模型的验证器相关的独特局限性，为设计更可靠的奖励系统提供了信息。\n训练人工智能系统执行任务的难易程度与任务的可验证程度成正比 Generative Rewards 这里着重说明 Generative Rewards for Non-Verifiable Tasks Reasoning Reward Models (Learning to Think)\nRubric-based Rewards (Structuring Subjectivity)：强调细粒度奖励\nCo-Evolving Systems (Unifying Policy and Reward):\nSelf-Rewarding：自我奖励，一个模型既充当policy model，也充当reward model\nCo-Optimization：policy和reward模型共同训练\nDense Rewards dense rewards 和 verifiers对于open-domain还是太难了\nScaling remains challenging for tasks like open-domain text generation due to the difficulty of defining dense rewards or using verifiers.\nGranularity细粒度分为：Trajectory（整个序列），Token，Step，Turn（Agent）\n这几个信号可以相互转换，比如把globa returns 转换成 localized signals，奖励的重新分配\n可以理解为从每次交互中直接分配和从结果中分解得出的回合级别奖励\nUnsupervised Rewards cluster：聚类（也有集群的意思）\nmajority vote：多数投票\nHeuristic Rewards（启发式奖励）：这种方法构成了另一种基于规则的奖励形式，采用基于输出属性（如长度或格式）的简单预定义规则作为质量的代理。由DeepSeek-R1开创。不会提高模型真正能力？（质疑）\nRewards Shaping Rewards Shaping将稀疏信号丰富为稳定的、信息丰富的梯度，用于LLM训练\nRule-based Reward Shaping\n最简单的：把rule-based verifier和reward model组合起来生成overall reward signal。通常有一个constant coefficient平衡the contributions of the reward model and the rule-based component，不是所有正确的responses都是同样的scores，这样可以把所有的responses重新排序，避免无效的学习梯度\n这种启发式组合策略在开放域任务中得到了广泛的应用，提供了更多的信息和有效的奖励信号。\n另一种方法是DeepSeek-R1中实现结果级奖励和格式奖励，能让LLM学习长思维链推理，用于解决LLM输出中的各种异常。\nStructure-based Reward Shaping Pass@k在推导和分析优势和有效近似值时，将集合级目标分解回单个样本信用分配。\nPolicy Optimization 最近的研究把on-policy RL和offline datasets结合在一起来进行optimization同时使用各种regularization techniques（正则化技术）比如entropy和KL防止overfitting\nenvironment:the context in RL for LLMs policy:the distribution of the next-level prediction 由于LLM中的大量参数，LLM的RL策略优化算法大多是基于一阶梯度的算法 Notations（符号） 当前状态s的预期积累奖励表示为V（value）函数 当前状态动作对应的预期积累表示为Q（quality）函数 优势函数：A(s, a) = Q(s, a) − V (s).该优势衡量的是与现有政策相比，当前行动在预期总回报方面有多大改进。 优化算法的历程 PPO算法[Schulman等人，2017b]首次被提出作为TRPO算法[Schurman等人，2015a]的计算高效近似。\nCritic-based Algorithms critic需要和LLM一起run和update，这样会导致巨大的计算开销，而且对于复杂的任务来说，扩展性不好\nCritic-Free Algorithms 只需要sequence-level rewards for training\n对于RLVR任务可以防止reward hacking等问题，这使得Critic-Free Algorithms更具有可扩展性\n最近的研究表明response-level足以用于RL的可扩展推理任务\n最受欢迎的critic-free approach是GRPO，新推出的GSPO[Zheng等人，2025a]，用序列级剪切代替了逐符号剪切的重要性采样率\nSPO引入了一种无组、单流策略优化，用持久的KL自适应值跟踪器和全局优势归一化来替换每组基线，从而产生比GRPO更平滑的收敛和更高的精度\nImportance Sampling for Policy Optimization TRPO引入了RL中重要性抽样的第一个版本，其中在目标中引入了令牌式重要性比wi，t\n这种方法在最近的工作中被广泛采用，如GRPO。由于无法在CoT的长上下文中有效计算实际分布比率，因此这种方法仅限于token-level重要性比率\ntoken-level重要性采样在RL算法中引入了另一种偏差，因为实际采样分布给定的策略是针对状态-动作对定义的，而token-level方法只考虑当前动作。GMPO[赵等人，2025f]通过引入几何平均来寻求缓解，以提高具有极端重要性采样率的token的训练鲁棒性\n在GSPO的最新工作中[Zheng等人，2025a]，计算了序列级重要性抽样因子。GSPO添加了一个唯一的归一化因子，以确保可以计算概率比，但这种方法也是对实际重要性抽样因子的有偏估计\nOff-policy Optimization Off-policy RL通过把data collection 和 policy learning解耦，实现了从历史、异步或离线数据集进行训练，从而提高了样本效率\n选择性地replay 早期的推理traces可以提高exploration for LLM reasoning\n现在很多都在对数据进行处理来提升exploration\nRegularization Objectives Objective-specific regularization helps balance explration and exploitation,boosting RL effiency and policy performance.\nKL,entropy and length regularization remain open questions,each affects policy optimization and scalability\nLength Penalty建议应用基于问题难度的自适应长度惩罚来保持模型的能力\nSampling Hyper-parameters Exploration and Exploitation Dynamics 一些工作提出了一种动态方法，例如分阶段提高温度(e.g., 1.40 → 1.45 → 1.50 for a 4B model, 0.7 → 1.0 → 1.1 for a 7B model)\nentropy 在0.3被发现是最佳平衡\n其他的工作只是倡导提高一个固定的温度（例如1.0或1.2）来鼓励初步探索，同时指出它本身不足以防止长期的熵下降\nLength Budgeting and Sequence Management 几乎所有的work都在努力管理生成响应的长度，以平衡性能和成本。\nThis involves starting RL with a short context window (e.g., 8k) before progressively increasing it to 16k, 24k, or 32k in later stages 初始的短上下文阶段被认为是必不可少的，因为它迫使模型学习更简洁、更具令牌效率的推理模式\nFoundational Problems RL’s Role: Sharpening or Discovery RL vs. SFT: Generalize or Memorize 发现RL擅长巩固和增强现有能力，而SFT在引入新知识或新模型能力方面更有效\nModel Prior: Weak and Strong R1-Zero：直接将大规模基于规则的RL应用于基本模型，产生新兴的长期推理\nR1：包含冷启动\n基础模型先验比指导模型更适合强化学习，通常会产生比从高度一致的指导模型开始时观察到的更平滑的改进轨迹，其中根深蒂固的格式和服从先验可能会干扰奖励的形成\nModel Family Differences Reward Type: Process or Outcome Training Resources Static Corpus Math\nCode\nSTEM\nAgent\nMixture\nRL Infrastructure \u0026amp; Framework e.g.OpenRLHF/veRL/AReaL/slime/TRL\nDynamic Environment Rule\nCode\nGame\nModel\nEnsemble\nApplications 1.Agentic Tasks\n2.Coding Tasks\n3.Multimodal Tasks\n4.Robotics Tasks\n5.Multi-Agent Systems\n6.Medical Tasks\n","date":"2025-09-24T00:00:00Z","image":"http://localhost:1313/p/%E5%A4%A7%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0%E7%B2%BE%E8%AF%BB/sky_hu_39929c5450262a55.jpg","permalink":"http://localhost:1313/p/%E5%A4%A7%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0%E7%B2%BE%E8%AF%BB/","title":"大型推理模型的强化学习综述精读"},{"content":"\nPass@k Training for Adptively Balancing Eplortion and Exploitation of LRMs Pass@k论文解读 policy探索能力的指标：the natural prevention of the decrease in the entropy of policy distribution\nPass@1和Pass@k之间的主要区别在于奖励计算和优势估计过程。\nvanilla：原来的\nGround Truth是正确答案（标准答案）\n探索能力: the entropy of policy distribution处在一个较高的水平\nthe answer diversity of the negative response处在一个较高的水平\nPass@k的entropy在RLVR procedure的200step左右开始上升\nk值影响Pass@k训练 实验中从4，8，16中调整k值\n无论k的值是多少，随着训练的进行，训练奖励都可以提高到相对较高的水平，这表明k的值不是帮助LLMs逃脱局部最优解的关键性因素\nk值越大，训练效率越慢（k值越大，优势值越小，导致优化步骤越短，训练效率越低） 训练效率的影响因素 实验在N = 32 和 k = 8的设置下使用{1 x 10-6,2 x 10-6,4 x 10-6}learning rate\n随着学习率的提高，拐点出现得更早，表明训练效率更高 最大优势值不是帮助模型表现优异的关键性因素 为更难的问题分配更大的优化强度可以有效地提高训练效率 根据the entropy of policy distribution的高低，区分high-exploration和low-exploration。\nhigh-exploration使用Pass@1 advantage fuction来exploit prior exploration，low-exploration使用Pass@k advantage function来encourage further exploration。\nImplicit reward design（隐式奖励设计）可以控制优化过程 具体地说，结合或动态调整不同形式的优势估计，可以同时提高exploration and exploitation能力\nPass@k论文复现 arxiv文章👈\ngithub仓库👈\nDatasets👈\nMaze 每个迷宫由文本表示，包含n行和n列，总共n∗n个字符\n四个字符“S”、“E”、“.”和“*”中的一个，分别表示起点、目的地、可用地点和不可用地点\n给定迷宫，LLM可以首先生成思维或推理过程，然后生成最终答案，其中包括四个动作“U”、“D”、“L”和“R”中的一个，分别表示向上、向下、向左和向右移动\n对于训练数据，我们构建了大小为9×9、11×11、13×13和15×15的迷宫，以增加训练数据的多样性\n对于测试数据，为了评估RLVR过程的泛化能力，我们不仅使用训练数据集进行相同大小的迷宫，还收集了大小为7×7、17×17、19×19和21×21的迷宫\n为了确保实验的有效性，我们在生成训练和测试数据后进行了严格的重复数据删除操作\nTraining Set 都是10,000;Test Set除了7 * 7,剩下的都是100\nImplementation Details Training backbone model:Qwen2.5-7B-Instruct和Qwen2.5-32B Instruct\nDAPO\nεlow=0.2和εhigh=0.28\ntoken-level policy gradient loss\nremove other optimizations\nlearning rate:1 × 10−6\nwarmup:10\nprompt batch size(BS prompt):128\nPrompt Batch Size：是模型推理生成的粒度。它决定在一次性并行处理多少个独立的提示词（例如，多少个用户问题），并同时为它们生成文本 mini-batch size(BS mini):32\nMini-batch Size：是模型权重更新的粒度。它决定在计算一次梯度下降时，使用多少条训练数据 rollout times:32\npositive reward Rpos = 1\nnegative reward Rneg = 0\ndo not employ any regularization methods, such as KL or Entropy regularization\ntemperature:1.0\nTop_P:0.95\n从概率最高的词开始累加它们的概率，直到累积概率达到或刚刚超过你设定的 top_p 值 For each question,we sample 32responses for Maze task and sample 8 responses for other tasks\nVersions/Dependencies Python 3.10.18\nRay 2.49.1\ngrpcio 1.75.0\nUbuntu 24.04.2 LTS\n如何从huggingface上下载数据集和模型 从huggingface上下载文件有2种方式，一种是直接登录后在网页上下载；一种是通过huggingface-cli命令下载。\n本文介绍的是第二种下载方式。\n安装 对于huggingface-cli命令的下载直接通过pip命令安装即可：\npip install -U huggingface_hub[hub_transfer]\n对于国内用户还可以通过设置镜像网站的方式加速下载：\n#linux export HF_ENDPOINT=https://hf-mirror.com\n#windows\nset HF_ENDPOINT=https://hf-mirror.com\n使用命令行下载\n模型 huggingface-cli download \u0026ndash;resume-download [1] \u0026ndash;local-dir [2] \u0026ndash;local-dir-use-symlinks False\n数据集 huggingface-cli download \u0026ndash;repo-type dataset \u0026ndash;resume-download [3] \u0026ndash;local-dir [4] \u0026ndash;local-dir-use-symlinks False \u0026ndash;token hf_***\n格式为：[1]和[3]表示项目的路径，格式为用户名/项目，比如mistralai/Mistral-7B-Instruct-v0.2表示的是mistralai下的7B instruct v0.2权重。[2]和[4]表示的是本地的保存地址。\n需要的注意的是有些仓库需要登录才可以下载，形如–token hf_***为huggingface的token配置。token的生成需要在huggingface个人页面生成.\n","date":"2025-09-21T16:00:00+08:00","image":"http://localhost:1313/p/pass@k%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E5%8F%8A%E5%85%B6%E5%A4%8D%E7%8E%B0/ByteDance_hu_b5130faa32e5aa6a.jpg","permalink":"http://localhost:1313/p/pass@k%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E5%8F%8A%E5%85%B6%E5%A4%8D%E7%8E%B0/","title":"Pass@k论文精读及其复现"},{"content":"Abstract 此文章发布约一周前就已经发现该问题了，但是由于专注看官方文档和仓库进行规范 + 课内事情，一直没有得到解决。\n在发文前一天发现此问题需要重点解决（无法避免），询问了师兄（论文作者）并咨询了相关团队，未果，所以花了两整天才解决这一个bug。\nIssue What happened + What you expected to happen Running the following snippet will hang indefinitely\n1 2 3 \u0026gt;\u0026gt;\u0026gt; import ray \u0026gt;\u0026gt;\u0026gt; ray.init() 2025-09-20 11:44:47,741 INFO worker.py:1538 -- Started a local Ray instance. Sometimes it will fail instead\n1 [2025-09-20 11:50:22,050 E 31652 31652] core_worker.cc:179: Failed to register worker 01000000ffffffffffffffffffffffffffffffffffffffffffffffff to Raylet. IOError: [RayletClient] Unable to register worker with raylet. No such file or directory Versions/Dependencies Python 3.10.18\nRay 2.49.1\ngrpcio 1.75.0\nUbuntu 24.04.2 LTS\nReproduction script 1 2 import ray ray.init() Issue Severity High: It blocks me from completing my task.\n上面全英是因为当时要提issue或者给Ray框架作者发邮件，但是后来解决了，就打算留下来了，这样后来的人可以模仿一下这个写法。\n可能的问题 1.workers实际上并没有启动。（可以看一下/tmp/ray/session_latest/raylet.out,如果在/tmp/ray/session_latest/看到有前缀python-core-worker- 的可以看一下，因为这个能了解工作进程可能发生了什么） 2.系统中进程数/线程数设置错误（可以通过cat /proc/sys/kernel/threads-max查看系统中一个进程可以创建多少个线程） 可能的方法 1.在import ray之后加上ray.init(num_cpus=56, num_gpus=2)。具体参数需要根据服务器进行自定义。 作者根据这个方法对自己进行了适配解决了问题。 具体操作：/yhy/verl/trainer/config/ppo_trainer.yaml配置文件中对num_cpus=0修改成num_cpus=10, num_gpus=1进行定义。 2.升级grpcio,2023年的时候安装grpcio 1.48.1 版本是有用的,相应的venv是 CentOS 7,Python 3.7.11，Ray 2.5.1,grpcio1.48.1。\n但我进行升级的时候，无法解决该问题，并且会导致包之间的冲突。（是一个opencv的包，已经pip install了） 3.添加ulimit -n 65536语句，因为分布式训练一开始可能会开成千上万个进程，默认是4096，所以会导致线程创建失败。 感觉这点也是有用的，但可能不是主要因素？ 4.一定要设置参数,只是用ray.init()就会崩溃。需要手动设置num_cpus。\n这点和1重复了，可以说是大家实验得到的结论？（也许） 近期其他人也遇到过该问题 2024.1.16也有在Ubuntu 20.04上遇到同样的问题 venv: ray == 2.7.1,grpcio == 1.59.2,python == 3.11.5\n2024.4.2有在ubuntu 22.04.3（docker内部）上遇到同样的问题，但是他只是失败，而不是挂起。在docker之外运行良好（他的M1 MacBook上）\n2024.4.14，2024.4.17，2024.7.11等等太多人遇到同样的问题了\n至此，问题的解决方案已经讲述完毕。 回顾解决问题的流程 刚遇到这个问题，我先看了一下是不是自己遇到过的，发现没有就交给了copilot，发现copilot无法解决，给了chatgpt5，同样无法解决，又给了Gemini看看能不能有些新意（其实这步可以忽略），上述方法都不行，问了师兄是否遇到过。\n发现他们都没有遇到过，我只能去Ray官方仓库里面的issue进行查看。感觉现在人们都不怎么用StackOverflow等等论坛了，所以就只能去issue里面找了。\n幸运的是发现了很多人遇到了同样的问题和报错，我就开始追根溯源，发现从17.18年就有人提出了这个问题，当时也有相应的解决办法，但是随着版本更新变得不适用。\n我就开始收集所有对这个问题的理解和解决方案，逐个尝试，很幸运的是我debug成功了！\n因为论文中没有常见的问题的解决方案，如果有的话应该是第一步先去看的。\n这就是我整个解决这个问题的流程，大体上看似乎没有太大问题。但是还是可以优化一下，下次遇到类似比较“偏”的问题可以更快，心态更平和地解决这个问题。\n看到这里了，祝你遇到像我遇到的这样比较“偏门”的问题时，也可以顺利并更快地解决！ ","date":"2025-09-20T10:00:00+08:00","image":"http://localhost:1313/p/ray.init%E5%88%9D%E5%A7%8B%E5%8C%96%E6%8C%82%E8%B5%B7/%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/Ray_hu_8d061ec42dec3d1e.jpg","permalink":"http://localhost:1313/p/ray.init%E5%88%9D%E5%A7%8B%E5%8C%96%E6%8C%82%E8%B5%B7/%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/","title":"ray.init()初始化挂起/失败问题的解决"},{"content":"\n科研日记 用来记录我远程实习的日子\n2025.07.16 成为赵老师的本科实习生，暑假跟着师兄做了一个横向\n家里网好差（本来服务器就慢），还是要在学校多用功 暑假一共6周，还有各种事情，加上休息吃饭探亲balabala也做不了太多事情。确定了方向，看了一些知乎上的论文解读（后面发现如果要通透还是要看原文）。\n2025.09.16 近期在搞论文复现的实验，跟上课题组的进度\n今天在修改run_dapo.sh脚本的时候发现了几个常见的问题\n.sh的格式十分严格 Parquet文件格式 Parquet文件格式讲解👈\nvibe coding脚本是无比正确的。\n不知不觉已经两个月了，但对我而言收获是颇多的。\n选择一个好组是十分重要的！软院TIC，高瓴AI BOX让我深刻感受到了好的氛围，成套的培养体系！ 希望自己可以平衡好课内＋科研，还有生活等等其他方面 晚上睡前简单看了一下verl官方文档里面Config Explanation的Data部分\n梯度下降的三种常见形式 1.Batch Gradient Descent（批量梯度下降） 2.Stochastic Gradient Descent (SGD，随机梯度下降) 3.Mini-batch Gradient Descent（小批量梯度下降） 几个epoch就是过几次数据集，实践中把 “batch” 这个词用得比较宽：在框架/代码里 batch_size 常指 mini-batch 的大小，所以容易混淆 “batch gradient descent” 与 “mini-batch”。\nprompt_key 这个不是学术上固定的术语，而是很多框架（比如 HuggingFace、VERL、LangChain 等）里常见的实现细节。\n在 字典 / JSON / 配置文件 里，用来标记某个 prompt 的 键名（key）。 这样做可以在代码里快速查找/复用不同的 prompt 模板。 1 2 3 4 5 prompts = { \u0026#34;translation\u0026#34;: \u0026#34;Translate the following English text into Chinese: {text}\u0026#34;, \u0026#34;summarization\u0026#34;: \u0026#34;Summarize the following paragraph: {text}\u0026#34;, \u0026#34;qa\u0026#34;: \u0026#34;Answer the question based on the context: {context}\\nQuestion: {question}\u0026#34; } 这里 \u0026ldquo;translation\u0026rdquo;, \u0026ldquo;summarization\u0026rdquo;, \u0026ldquo;qa\u0026rdquo; 就是 prompt_key， 而它们对应的 value 就是具体的 prompt 模板。\nPrompt = 给模型的输入提示，引导它完成任务。\nPrompt_key = 在程序里标记或索引 prompt 模板的“名字/键”，方便管理和调用。\nRM（Reward Model，奖励模型）：在 RLHF 里给生成结果打分的模型。\n如果使用基于模型的 RM，并且策略和 RM 的聊天模板不同，则需要设置data.return_raw_input_ids=True data.return_full_prompt=True 用户输入：你好，介绍一下强化学习 返回：[INST] 你好，介绍一下强化学习 [/INST] data.return_raw_chat=True 用户输入：你好，介绍一下强化学习 返回的就是:你好，介绍一下强化学习\n2025.09.17 早晨起来去工位继续看verl\nactor_rollout_ref.hybrid_engine：是否是混合引擎，目前只支持混合引擎.\nDropout 是一种 正则化方法，用来防止神经网络过拟合。在训练时，随机“丢弃”一部分神经元（让它们暂时不参与计算和更新）。推理时,不再丢弃任何神经元，只是使用完整的输出。\nactor_rollout_ref.model.use_remove_padding一般都选true移除\u0026lt;PAD\u0026gt;来加速推理，但是多模态或者大工程里仍有人使用false\nTemperature （温度）。T = 1 → 正常分布。T \u0026gt; 1 → 分布更平滑，增加随机性，容易生成多样化甚至跑偏的内容。T \u0026lt; 1 → 分布更尖锐，模型更确定（更倾向选概率最高的 token，输出保守）。\nTop-k：从 softmax 排序后的前 k 个 token 中随机抽样。\nTop-p：动态选择前 累计概率 ≥ p 的最小 token 集合，从里面采样。\nActor:负责 更新参数\nRef (Reference Model):负责 对比/约束。它是冻结的（不更新），通常是最初的预训练模型。\nRollout:负责 产生输出（推理采样）\nEOS = End Of Sequence（序列结束标记）。在 tokenizer 里，EOS 往往是个特殊的 \u0026lt;/s\u0026gt; 或 \u0026lt;eos\u0026gt; 符号。ignore_eos=True 在训练中一般少用，除非你需要 生成固定长度序列，或者想收集超过 EOS 的 rollouts 数据。\n2025.09.19 一个深度学习训练任务中，nodes 指的是计算机，而 gpus-per-node 指的是每台计算机上安装的 GPU 数量。\n可以把 nodes 理解为一台台服务器，每台服务器里可以插上多张显卡（GPU）。\nnnodes: 1：你正在使用一台计算机来运行任务。\nn_gpus_per_node: 8：这台计算机上插了 8 张 GPU。\n所以，这个配置的意思是，你用一台装有 8 张 GPU 的服务器来运行你的任务。\nverl官方文档看完了，嗯。。还是要去看仓库\n都在赶iclr导致服务器又变得卡卡的\n实验复现遇到了问题导致一直是卡住的状态\n神器：\npkill -9 -u $USER -f ray\n2025.09.20 ray官方仓库，试图解决Ray实例后ray.init()挂起/失败问题\nimport ray ray.init() 2025-09-20 11:44:47,741 INFO worker.py:1538 \u0026ndash; Started a local Ray instance.\n有时候会卡住\npython import ray ray.init() 2025-09-20 11:44:47,741 INFO worker.py:1538 \u0026ndash; Started a local Ray instance.\n有时候会失败\n[2025-09-20 11:50:22,050 E 31652 31652] core_worker.cc:179: Failed to register worker 01000000ffffffffffffffffffffffffffffffffffffffffffffffff to Raylet. IOError: [RayletClient] Unable to register worker with raylet. No such file or directory\nVersions/Dependencies Python 3.10 Ray grpcio OS:\nReproduction script import ray ray.init()\nIssue Severity High:It blocks me from completing my task.\n可能的原因:1.workers实际上没有启动 2.系统中一个进程可以创建多少个线程？（可以通过cat /proc/sys/kernel/threads-max查看）\n可能的方法:1.在import ray后添加 ray.init(num_cpus=56, num_gpus=2) 这个方法很多人似乎有帮助，但不是一个好的解决方案 2.可以看一下/tmp/ray/session_latest/raylet.out,如果在/tmp/ray/session_latest/看到有前缀python-core-worker- 的可以看一下，因为这个能了解工作进程可能发生了什么 3.升级grpcio,2023年的时候安装grpcio 1.48.1 版本是有用的,相应的venv是 CentOS 7,Python 3.7.11，Ray 2.5.1,grpcio1.48.1 4.\n#!/bin/bash ulimit -n 65536 python3 -m verl.trainer.main_ppo \u0026hellip;\n5.一定要设置参数,只是用ray.init()就会崩溃。需要手动设置num_cpus\n其他人： 2024.1.16也有在Ubuntu 20.04上遇到同样的问题 venv: ray == 2.7.1,grpcio == 1.59.2,python == 3.11.5 2024.4.2有在ubuntu 22.04.3（docker内部）上遇到同样的问题，但是他只是失败，而不是挂起。在docker之外运行良好（他的M1 MacBook上）\n2024.4.14，2024.4.17，2024.7.11等等太多人遇到同样的问题了\n15点左右,/yhy/verl/trainer/config/ppo_trainer.yaml配置文件中进行修改\n问题已解决！在import ray后添加 ray.init(num_cpus=56, num_gpus=2) 自己看issue，扒仓库源码等等解决了这个问题\n2025.10.22 结束了第一段实习，收获满满！\n谢谢师兄！谢谢赵老师！\n期待后续再合作！\n","date":"2025-09-16T10:00:00+08:00","image":"http://localhost:1313/p/ruc-study-framework/RUC_hu_19ceed1f7aef8036.jpg","permalink":"http://localhost:1313/p/ruc-study-framework/","title":"RUC科研之旅"},{"content":"欢迎你能来到我的第一篇文章！\n为什么要写博客？ 写博客对我来说有着特殊的意义：\n对自己 记录笔记：把学到的东西都记下来，以后可以复习 积攒经验：积攒宝贵的经验 记录进步：记录自己的进步 对他人 感谢师父：感谢文聪学长的帮助，永远的师傅！ 传承精神：希望后来者能够更快速地入门 分享内容：希望其他人能够更快地了解我 博客内容规划 我计划我的文章中记录以下内容：\n笔记 (Notes) 可能比较杂，什么都有。但重点是关于强化学习的内容\n理论 (Theory) 一些理论知识\n日记 (Diary) 记录一些自己的日常，探讨人生\n结语 写博客是一个长期的过程，也应该是一个很开心的过程！\n这篇文章写于 2025年9月15日，是我博客的第一篇文章。希望它能成为一个美好的开始。\n","date":"2025-09-15T00:00:00Z","image":"http://localhost:1313/p/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/first-blog-cover_hu_19f8cf5e7cdc0c7d.jpg","permalink":"http://localhost:1313/p/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/","title":"第一篇文章"}]