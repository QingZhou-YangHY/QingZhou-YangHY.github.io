[{"content":"","date":"2025-10-02T00:00:00Z","image":"http://localhost:1313/p/still3%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/1_hu_c1b6fcb8e3828ba7.jpg","permalink":"http://localhost:1313/p/still3%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/","title":"STILL3论文精读"},{"content":"强化学习的流程 加载模型（无需修改）\nveRL使用Ray框架进行模型的调度\ninit_workers 初始化策略模型、参考模型、价值模型、奖励模型。这里面有些模型不会用到。很多都不会使用价值模型，奖励模型也都是基于规则的 训练过程\nget_data_batch(无需修改) generate_sequences(normal-rl和agent-rl) normal-rl：单轮的交互（prompt\u0026ndash;\u0026gt;model\u0026ndash;\u0026gt;response），这个流程就结束了\nagent-rl：多轮的交互（prompt\u0026ndash;\u0026gt;model\u0026ndash;\u0026gt;response\u0026ndash;\u0026gt;env（tool call,code exec ,etc）\u0026ndash;\u0026gt;model\u0026ndash;\u0026gt;response\u0026ndash;\u0026gt;\u0026hellip;）\nreward（通过自定义的奖励函数控制，一般无需修改）不用去修改veRL框架，通过配置文件进行修改\nlog_probs（计算策略模型和参考模型输出token的概率，一般无需修改）\nvalues(目前主流的rl算法基本上都在舍弃价值模型，一般无需修改)\nadv(各rl算法之间的差异主要体现在adv的计算方式，此处需要针对不同的算法进行修改) compute_loss（normal-rl和agent-rl） normal-rl：无需和环境交互，response中token全部由模型生成，均参与损失计算\nagent-rl：需要获取观测结果，观测结果部分不是模型生成，计算损失时需要对其进行mask\nupdate actor（无需修改）\n","date":"2025-10-01T00:00:00Z","image":"http://localhost:1313/sky.jpg","permalink":"http://localhost:1313/p/verl%E6%B2%89%E6%B7%80%E6%9A%82%E6%9C%AA%E6%83%B3%E5%A5%BD%E5%90%8D%E5%AD%97/","title":"veRL沉淀（暂未想好名字）"},{"content":"A Survey of Reinforcement Learning for Large Reasoning Models Background RL在推进LLM能力的前沿方面取得了显著成功，特别是在解决数学和编码等复杂逻辑任务方面。因此，RL已成为将LLM转化为LRM的基础方法。\n需要探索提高强化学习向人工超级智能（ASI）的可扩展性的策略。\n回顾一下大致流程吧！\n简单讲解一下RL应用到语言模型的时候，这些概念映射到了哪里 Prompt/Task（x）：对应于初始状态或环境上下文，从数据分布中提取，对应于数据集D。\nPolicy (πθ):表示语言模型，它根据提示生成一个长度为T的序列，表示为y=（y1，…，yT）。\nState (st):定义为提示以及到目前为止生成的令牌，即st=（x，a1:t−1）。\nAction (at):在步骤t从动作空间A中选择的单元。根据粒度，动作可以是整个序列y（序列级）、∈V处的令牌（令牌级）或 片段\nTransition Dynamics (P):在LLM的上下文中，状态转换通常是确定的，因为st+1=[st，at]，其中[·，·]表示字符串连接。当状态包含EOS令牌时，策略将转换为终端状态，这意味着轨迹结束。\nReward (R(x, y) or rt):基于动作粒度进行分配，例如，轨迹末端的序列级别R（x，y），每个令牌的令牌级别rt=R（x、a1:t），或每个分段的步长级别rk=R（x、y（1:k））。\nReturn (G):提示x的整个轨迹y的累积奖励（通常在有限时间内γ=1）。它通过序列级奖励简化为单个标量R（x，y），否则按每个令牌/步骤聚合奖励\nFrontier Models 按时间顺序排列在三个主要方向上：LRM、agentic LRMs和多模态LRM。\n一个大型推理模型，OpenAI的o1[2024]系列，建立了将训练时间RL和测试时间计算扩展到更强大的推理能力的有效性，在数学、编码和科学基准测试方面取得了领先成果。\nDeepSeek的旗舰模型R1[2025a]是第一个在基准测试中与o1性能相匹配的开源模型。它采用多阶段训练管道来确保全面的模型能力，并探索了没有监督微调的纯RL路线（即Zero RL）。\n其他专有模型发布紧随其后：Claude-3.7-Sonnet[2025a]以混合推理为特色，Gemini 2.0和2.5[2025]引入了更长的上下文长度，Seed Thinking 1.5[2025b]以跨领域的泛化为特色，o3[2025a]系列展示了越来越先进的推理能力。最近，OpenAI推出了他们的第一个开源推理模型gpt-oss-120b[2025a]，随后推出了GPT5[2025a]，这是他们迄今为止最强大的人工智能系统，可以在高效模型和更深入的推理模型gpt-5思维之间灵活切换。并行的开源努力继续扩大了格局。在Qwen家族中，QwQ-32B[2025g]与R1的表现相匹配，其次是Qwen3[2025a]系列，代表性型号Qwen3-235B进一步提高了基准分数。Skywork-OR1[2025d]模型套件基于R1蒸馏模型，并通过有效的数据混合和算法创新实现了可扩展的RL训练。Minimax-M1[2025a]是第一个有效地将混合注意力引入尺度RL的模型。其他作品包括Llama Nemotron Ultra[2025]，旨在平衡准确性和效率；Magistral 24B[2025]，通过RL从头开始训练，而不是从先前的模型中提炼；以及种子OSS[2025a]，强调长上下文推理能力。等等\u0026hellip;\n过去的一年发展迅速啊！\n1. Foundational Components 1.1 Reward Design 在1.1中，我们对LRM RL中的奖励设计进行了全面的考察。从可验证的奖励开始，DeepSeek-R1的成功就是例证，它通过可验证的奖励机制证明了RL的可扩展性。\n在1.2中，我们考察生成性奖励，其中模型用于验证或直接生成奖励信号。\n然而，可验证和生成性奖励通常都表示为稀疏的数值反馈。一个重要的互补维度在于奖励信号的密度。\n1.3相应地考察了包含密集奖励的方法。另一个分类轴涉及奖励是根据外部真实情况计算的，还是由模型直接估计的。\n这一区别促使我们在1.4中讨论无监督奖励。\n在这四个类别的基础上，我们在1.5中转向奖励塑造，在那里我们分析了组合或转换不同奖励信号以促进学习的策略。\n1.1.1Verifiable Rewards 基于规则的奖励通过利用准确性和格式检查，为RL提供可扩展和可靠的训练信号，特别是在数学和代码任务中。\nVerifier定律强调，具有清晰和自动验证的任务可以实现高效的RL优化，而主观任务仍然具有挑战性。\nRule-based Rewards Accuracy rewards:对于具有确定性结果的任务（例如数学），策略必须在规定的分隔符（通常为\\boxed{…}）内产生最终解决方案。然后，自动检查器将此输出与地面实况进行比较。对于编码任务，单元测试或编译器提供通过/失败信号\nFormat rewards:这些奖励施加了一个结构约束，要求模型将其私有思想链放置在和之间，并在单独的字段中输出最终答案（例如…）。这提高了大规模RL中的可靠解析和验证\nRule-based Verifier 基于规则的奖励通常来自基于规则的验证器。这些依赖于大量手动编写的等价规则来确定预测的答案是否与基本事实相匹配。目前，广泛使用的数学验证器主要基于Python库Math-Verify1和SymPy2构建。此外，一些作品，如DAPO[2025d]和DeepScaleR[2025c]，也提供了开源和成熟的验证器。最近，Huang等人[2025e]强调了与基于规则和基于模型的验证器相关的独特局限性，为设计更可靠的奖励系统提供了信息。\n训练人工智能系统执行任务的难易程度与任务的可验证程度成正比 1.1.2Generative Rewards 写于2025/9/24晚21点56分，教学楼关门故回寝室。\n2.Generative Rewards\n3.Dense Rewards\n4.Unsupervised Rewards\n5.Rewards Shaping\nPolicy Optimization\n1.Policy Gradient 2.Critic-Based Algorithms 3.Critic-Free Algorithms 4.Off-Policy Optimization 5.Regularization Objectives Sampling Strategy\n1.Dynamic Sampling 2.Sampling Hyper-Parameters Foundational Problems\n1.RL\u0026rsquo;s Role\nSharpening vs Discovery 2.Model Prior\nWeak vs Strong 3.Training Recipes\nTricks vs Traps 4.RL vs. SFT\nGeneralize vs Memorize 5.Reward Type\nProcess vs Outcome Training Resource\n1.Static Corpus Math Code STEM Agent Mixture 2.RL Infrastructure \u0026amp; Framework\ne.g.OpenRLHF/veRL/AReaL/slime/TRL 3.Dynamic Environment Rule Code Game Model Ensemble Applications\n1.Agentic Tasks 2.Coding Tasks 3.Multimodal Tasks 4.Robotics Tasks 5.Multi-Agent Systems 6.Medical Tasks ","date":"2025-09-24T00:00:00Z","image":"http://localhost:1313/p/%E5%A4%A7%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0%E4%B8%AA%E4%BA%BA%E8%A7%A3%E8%AF%BB/sky_hu_39929c5450262a55.jpg","permalink":"http://localhost:1313/p/%E5%A4%A7%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0%E4%B8%AA%E4%BA%BA%E8%A7%A3%E8%AF%BB/","title":"大型推理模型的强化学习综述个人解读"},{"content":"\nPass@k论文复现 Pass@k Training for Adptively Balancing Eplortion and Exploitation of LRMs arxiv文章👈\ngithub仓库👈\nDatasets👈\nVersions/Dependencies Python 3.10.18\nRay 2.49.1\ngrpcio 1.75.0\nUbuntu 24.04.2 LTS\n如何从huggingface上下载数据集和模型 从huggingface上下载文件有2种方式，一种是直接登录后在网页上下载；一种是通过huggingface-cli命令下载。\n本文介绍的是第二种下载方式。\n安装 对于huggingface-cli命令的下载直接通过pip命令安装即可：\npip install -U huggingface_hub[hub_transfer]\n对于国内用户还可以通过设置镜像网站的方式加速下载：\n#linux export HF_ENDPOINT=https://hf-mirror.com\n#windows\nset HF_ENDPOINT=https://hf-mirror.com\n使用命令行下载\n模型 huggingface-cli download \u0026ndash;resume-download [1] \u0026ndash;local-dir [2] \u0026ndash;local-dir-use-symlinks False\n数据集 huggingface-cli download \u0026ndash;repo-type dataset \u0026ndash;resume-download [3] \u0026ndash;local-dir [4] \u0026ndash;local-dir-use-symlinks False \u0026ndash;token hf_***\n格式为：[1]和[3]表示项目的路径，格式为用户名/项目，比如mistralai/Mistral-7B-Instruct-v0.2表示的是mistralai下的7B instruct v0.2权重。[2]和[4]表示的是本地的保存地址。\n需要的注意的是有些仓库需要登录才可以下载，形如–token hf_***为huggingface的token配置。token的生成需要在huggingface个人页面生成.\n","date":"2025-09-21T16:00:00+08:00","image":"http://localhost:1313/p/pass@k%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/ByteDance_hu_b5130faa32e5aa6a.jpg","permalink":"http://localhost:1313/p/pass@k%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/","title":"Pass@k论文复现"},{"content":"Abstract 此文章发布约一周前就已经发现该问题了，但是由于专注看官方文档和仓库进行规范 + 课内事情，一直没有得到解决。\n在发文前一天发现此问题需要重点解决（无法避免），询问了师兄（论文作者）并咨询了相关团队，未果，所以花了两整天才解决这一个bug。\nIssue What happened + What you expected to happen Running the following snippet will hang indefinitely\n1 2 3 \u0026gt;\u0026gt;\u0026gt; import ray \u0026gt;\u0026gt;\u0026gt; ray.init() 2025-09-20 11:44:47,741 INFO worker.py:1538 -- Started a local Ray instance. Sometimes it will fail instead\n1 [2025-09-20 11:50:22,050 E 31652 31652] core_worker.cc:179: Failed to register worker 01000000ffffffffffffffffffffffffffffffffffffffffffffffff to Raylet. IOError: [RayletClient] Unable to register worker with raylet. No such file or directory Versions/Dependencies Python 3.10.18\nRay 2.49.1\ngrpcio 1.75.0\nUbuntu 24.04.2 LTS\nReproduction script 1 2 import ray ray.init() Issue Severity High: It blocks me from completing my task.\n上面全英是因为当时要提issue或者给Ray框架作者发邮件，但是后来解决了，就打算留下来了，这样后来的人可以模仿一下这个写法。\n可能的问题 1.workers实际上并没有启动。（可以看一下/tmp/ray/session_latest/raylet.out,如果在/tmp/ray/session_latest/看到有前缀python-core-worker- 的可以看一下，因为这个能了解工作进程可能发生了什么） 2.系统中进程数/线程数设置错误（可以通过cat /proc/sys/kernel/threads-max查看系统中一个进程可以创建多少个线程） 可能的方法 1.在import ray之后加上ray.init(num_cpus=56, num_gpus=2)。具体参数需要根据服务器进行自定义。 作者根据这个方法对自己进行了适配解决了问题。 具体操作：/yhy/verl/trainer/config/ppo_trainer.yaml配置文件中对num_cpus=0修改成num_cpus=10, num_gpus=1进行定义。 2.升级grpcio,2023年的时候安装grpcio 1.48.1 版本是有用的,相应的venv是 CentOS 7,Python 3.7.11，Ray 2.5.1,grpcio1.48.1。\n但我进行升级的时候，无法解决该问题，并且会导致包之间的冲突。（是一个opencv的包，已经pip install了） 3.添加ulimit -n 65536语句，因为分布式训练一开始可能会开成千上万个进程，默认是4096，所以会导致线程创建失败。 感觉这点也是有用的，但可能不是主要因素？ 4.一定要设置参数,只是用ray.init()就会崩溃。需要手动设置num_cpus。\n这点和1重复了，可以说是大家实验得到的结论？（也许） 近期其他人也遇到过该问题 2024.1.16也有在Ubuntu 20.04上遇到同样的问题 venv: ray == 2.7.1,grpcio == 1.59.2,python == 3.11.5\n2024.4.2有在ubuntu 22.04.3（docker内部）上遇到同样的问题，但是他只是失败，而不是挂起。在docker之外运行良好（他的M1 MacBook上）\n2024.4.14，2024.4.17，2024.7.11等等太多人遇到同样的问题了\n至此，问题的解决方案已经讲述完毕。 回顾解决问题的流程 刚遇到这个问题，我先看了一下是不是自己遇到过的，发现没有就交给了copilot，发现copilot无法解决，给了chatgpt5，同样无法解决，又给了Gemini看看能不能有些新意（其实这步可以忽略），上述方法都不行，问了师兄是否遇到过。\n发现他们都没有遇到过，我只能去Ray官方仓库里面的issue进行查看。感觉现在人们都不怎么用StackOverflow等等论坛了，所以就只能去issue里面找了。\n幸运的是发现了很多人遇到了同样的问题和报错，我就开始追根溯源，发现从17.18年就有人提出了这个问题，当时也有相应的解决办法，但是随着版本更新变得不适用。\n我就开始收集所有对这个问题的理解和解决方案，逐个尝试，很幸运的是我debug成功了！\n因为论文中没有常见的问题的解决方案，如果有的话应该是第一步先去看的。\n这就是我整个解决这个问题的流程，大体上看似乎没有太大问题。但是还是可以优化一下，下次遇到类似比较“偏”的问题可以更快，心态更平和地解决这个问题。\n看到这里了，祝你遇到像我遇到的这样比较“偏门”的问题时，也可以顺利并更快地解决！ ","date":"2025-09-20T10:00:00+08:00","image":"http://localhost:1313/p/ray.init%E5%88%9D%E5%A7%8B%E5%8C%96%E6%8C%82%E8%B5%B7/%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/Ray_hu_8d061ec42dec3d1e.jpg","permalink":"http://localhost:1313/p/ray.init%E5%88%9D%E5%A7%8B%E5%8C%96%E6%8C%82%E8%B5%B7/%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/","title":"ray.init()初始化挂起/失败问题的解决"},{"content":"\n科研日记 用来记录我远程实习的日子\n2025.07.16 成为赵老师的本科实习生，暑假跟着师兄做了一个横向\n家里网好差（本来服务器就慢），还是要在学校多用功 暑假一共6周，还有各种事情，加上休息吃饭探亲balabala也做不了太多事情。确定了方向，看了一些知乎上的论文解读（后面发现如果要通透还是要看原文）。\n2025.09.16 近期在搞论文复现的实验，跟上课题组的进度\n今天在修改run_dapo.sh脚本的时候发现了几个常见的问题\n.sh的格式十分严格 Parquet文件格式 Parquet文件格式讲解👈\nvibe coding脚本是无比正确的。\n不知不觉已经两个月了，但对我而言收获是颇多的。\n选择一个好组是十分重要的！软院TIC，高瓴AI BOX让我深刻感受到了好的氛围，成套的培养体系！ 希望自己可以平衡好课内＋科研，还有生活等等其他方面 晚上睡前简单看了一下verl官方文档里面Config Explanation的Data部分\n梯度下降的三种常见形式 1.Batch Gradient Descent（批量梯度下降） 2.Stochastic Gradient Descent (SGD，随机梯度下降) 3.Mini-batch Gradient Descent（小批量梯度下降） 几个epoch就是过几次数据集，实践中把 “batch” 这个词用得比较宽：在框架/代码里 batch_size 常指 mini-batch 的大小，所以容易混淆 “batch gradient descent” 与 “mini-batch”。\nprompt_key 这个不是学术上固定的术语，而是很多框架（比如 HuggingFace、VERL、LangChain 等）里常见的实现细节。\n在 字典 / JSON / 配置文件 里，用来标记某个 prompt 的 键名（key）。 这样做可以在代码里快速查找/复用不同的 prompt 模板。 1 2 3 4 5 prompts = { \u0026#34;translation\u0026#34;: \u0026#34;Translate the following English text into Chinese: {text}\u0026#34;, \u0026#34;summarization\u0026#34;: \u0026#34;Summarize the following paragraph: {text}\u0026#34;, \u0026#34;qa\u0026#34;: \u0026#34;Answer the question based on the context: {context}\\nQuestion: {question}\u0026#34; } 这里 \u0026ldquo;translation\u0026rdquo;, \u0026ldquo;summarization\u0026rdquo;, \u0026ldquo;qa\u0026rdquo; 就是 prompt_key， 而它们对应的 value 就是具体的 prompt 模板。\nPrompt = 给模型的输入提示，引导它完成任务。\nPrompt_key = 在程序里标记或索引 prompt 模板的“名字/键”，方便管理和调用。\nRM（Reward Model，奖励模型）：在 RLHF 里给生成结果打分的模型。\n如果使用基于模型的 RM，并且策略和 RM 的聊天模板不同，则需要设置data.return_raw_input_ids=True data.return_full_prompt=True 用户输入：你好，介绍一下强化学习 返回：[INST] 你好，介绍一下强化学习 [/INST] data.return_raw_chat=True 用户输入：你好，介绍一下强化学习 返回的就是:你好，介绍一下强化学习\n2025.09.17 早晨起来去工位继续看verl\nactor_rollout_ref.hybrid_engine：是否是混合引擎，目前只支持混合引擎.\nDropout 是一种 正则化方法，用来防止神经网络过拟合。在训练时，随机“丢弃”一部分神经元（让它们暂时不参与计算和更新）。推理时,不再丢弃任何神经元，只是使用完整的输出。\nactor_rollout_ref.model.use_remove_padding一般都选true移除\u0026lt;PAD\u0026gt;来加速推理，但是多模态或者大工程里仍有人使用false\nTemperature （温度）。T = 1 → 正常分布。T \u0026gt; 1 → 分布更平滑，增加随机性，容易生成多样化甚至跑偏的内容。T \u0026lt; 1 → 分布更尖锐，模型更确定（更倾向选概率最高的 token，输出保守）。\nTop-k：从 softmax 排序后的前 k 个 token 中随机抽样。\nTop-p：动态选择前 累计概率 ≥ p 的最小 token 集合，从里面采样。\nActor:负责 更新参数\nRef (Reference Model):负责 对比/约束。它是冻结的（不更新），通常是最初的预训练模型。\nRollout:负责 产生输出（推理采样）\nEOS = End Of Sequence（序列结束标记）。在 tokenizer 里，EOS 往往是个特殊的 \u0026lt;/s\u0026gt; 或 \u0026lt;eos\u0026gt; 符号。ignore_eos=True 在训练中一般少用，除非你需要 生成固定长度序列，或者想收集超过 EOS 的 rollouts 数据。\n2025.09.19 一个深度学习训练任务中，nodes 指的是计算机，而 gpus-per-node 指的是每台计算机上安装的 GPU 数量。\n可以把 nodes 理解为一台台服务器，每台服务器里可以插上多张显卡（GPU）。\nnnodes: 1：你正在使用一台计算机来运行任务。\nn_gpus_per_node: 8：这台计算机上插了 8 张 GPU。\n所以，这个配置的意思是，你用一台装有 8 张 GPU 的服务器来运行你的任务。\nverl官方文档看完了，嗯。。还是要去看仓库\n都在赶iclr导致服务器又变得卡卡的\n实验复现遇到了问题导致一直是卡住的状态\n神器：\npkill -9 -u $USER -f ray\n2025.09.20 ray官方仓库，试图解决Ray实例后ray.init()挂起/失败问题\nimport ray ray.init() 2025-09-20 11:44:47,741 INFO worker.py:1538 \u0026ndash; Started a local Ray instance.\n有时候会卡住\npython import ray ray.init() 2025-09-20 11:44:47,741 INFO worker.py:1538 \u0026ndash; Started a local Ray instance.\n有时候会失败\n[2025-09-20 11:50:22,050 E 31652 31652] core_worker.cc:179: Failed to register worker 01000000ffffffffffffffffffffffffffffffffffffffffffffffff to Raylet. IOError: [RayletClient] Unable to register worker with raylet. No such file or directory\nVersions/Dependencies Python 3.10 Ray grpcio OS:\nReproduction script import ray ray.init()\nIssue Severity High:It blocks me from completing my task.\n可能的原因:1.workers实际上没有启动 2.系统中一个进程可以创建多少个线程？（可以通过cat /proc/sys/kernel/threads-max查看）\n可能的方法:1.在import ray后添加 ray.init(num_cpus=56, num_gpus=2) 这个方法很多人似乎有帮助，但不是一个好的解决方案 2.可以看一下/tmp/ray/session_latest/raylet.out,如果在/tmp/ray/session_latest/看到有前缀python-core-worker- 的可以看一下，因为这个能了解工作进程可能发生了什么 3.升级grpcio,2023年的时候安装grpcio 1.48.1 版本是有用的,相应的venv是 CentOS 7,Python 3.7.11，Ray 2.5.1,grpcio1.48.1 4.\n#!/bin/bash ulimit -n 65536 python3 -m verl.trainer.main_ppo \u0026hellip;\n5.一定要设置参数,只是用ray.init()就会崩溃。需要手动设置num_cpus\n其他人： 2024.1.16也有在Ubuntu 20.04上遇到同样的问题 venv: ray == 2.7.1,grpcio == 1.59.2,python == 3.11.5 2024.4.2有在ubuntu 22.04.3（docker内部）上遇到同样的问题，但是他只是失败，而不是挂起。在docker之外运行良好（他的M1 MacBook上）\n2024.4.14，2024.4.17，2024.7.11等等太多人遇到同样的问题了\n15点左右,/yhy/verl/trainer/config/ppo_trainer.yaml配置文件中进行修改\n问题已解决！在import ray后添加 ray.init(num_cpus=56, num_gpus=2) 今天最开心的事情，自己成功看issue，扒仓库源码等等解决了这个问题\n","date":"2025-09-16T10:00:00+08:00","image":"http://localhost:1313/p/ruc-study-framework/RUC_hu_19ceed1f7aef8036.jpg","permalink":"http://localhost:1313/p/ruc-study-framework/","title":"RUC科研之旅"},{"content":"欢迎你能来到我的第一篇文章！\n为什么要写博客？ 写博客对我来说有着特殊的意义：\n对自己 记录笔记：把学到的东西都记下来，以后可以复习 积攒经验：积攒宝贵的经验 记录进步：记录自己的进步 对他人 感谢师父：感谢文聪学长的帮助，永远的师傅！ 传承精神：希望后来者能够更快速地入门 分享内容：希望其他人能够更快地了解我 博客内容规划 我计划我的文章中记录以下内容：\n笔记 (Notes) 可能比较杂，什么都有。但重点是关于强化学习的内容\n理论 (Theory) 一些理论知识\n日记 (Diary) 记录一些自己的日常，探讨人生\n结语 写博客是一个长期的过程，也应该是一个很开心的过程！\n这篇文章写于 2025年9月15日，是我博客的第一篇文章。希望它能成为一个美好的开始。\n","date":"2025-09-15T00:00:00Z","image":"http://localhost:1313/p/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/first-blog-cover_hu_19f8cf5e7cdc0c7d.jpg","permalink":"http://localhost:1313/p/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/","title":"第一篇文章"}]